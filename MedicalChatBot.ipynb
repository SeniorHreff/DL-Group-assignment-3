{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.33.2 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off"
      ],
      "metadata": {
        "id": "qBs7EilSR0Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4k7Uk2MR0SG",
        "outputId": "f08f23b4-958e-4ca6-937e-241545f8b0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/277.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Llama 2 is as easy as using any other HuggingFace model. We'll be using the HuggingFacePipeline wrapper (from LangChain) to make it even easier to use. To load the 13B version of the model, we'll use a GPTQ (Generative Pre-trained Transformer Quantization) version of the model:"
      ],
      "metadata": {
        "id": "CCqSSJVFgEXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the relevant liberies\n",
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "##########################################################################################\n",
        "\n",
        "# We set the model name, create a tokenizer, and load the pre-trained language model for text generation.\n",
        "MODEL_NAME = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "##########################################################################################\n",
        "\n",
        "# Configure the text generation settings using GenerationConfig.\n",
        "#These settings include the maximum number of new tokens in the generated text, temperature (controls randomness), top-p sampling, text sampling, and repetition penalty.\n",
        "\n",
        "# Create a configuration for text generation based on the specified model name\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "# Set the maximum number of new tokens in the generated text to 1024.\n",
        "# This limits the length of the generated output to 1024 tokens.\n",
        "generation_config.max_new_tokens = 1024\n",
        "\n",
        "# Set the temperature for text generation. Lower values (e.g., 0.0001) make output more deterministic, following likely predictions.\n",
        "# Higher values make the output more random.\n",
        "generation_config.temperature = 0.0001\n",
        "\n",
        "# Set the top-p sampling value. A value of 0.95 means focusing on the most likely words that make up 95% of the probability distribution.\n",
        "generation_config.top_p = 0.95\n",
        "\n",
        "# Enable text sampling. When set to True, the model randomly selects words based on their probabilities, introducing randomness.\n",
        "generation_config.do_sample = True\n",
        "\n",
        "# Set the repetition penalty. A value of 1.15 discourages the model from repeating the same words or phrases too frequently in the output.\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "# We create a text generation pipeline using the configured model, tokenizer, and generation configuration.\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "##########################################################################################\n",
        "# We create a LangChain pipeline that wraps the Hugging Face text generation pipeline.\n",
        "# This LangChain pipeline is configured with a specific temperature setting (temperature: 0), which affects the randomness of the generated text.\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n"
      ],
      "metadata": {
        "id": "9NInr9AGXFAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To work with external files, LangChain provides data loaders that can be used to load documents from various sources. Combining LLMs with external data is generally referred to as Retrieval Augmented Generation (RAG).\n",
        "\n",
        "Let's see how we can use the UnstructuredMarkdownLoader to load a document from a Markdown file:"
      ],
      "metadata": {
        "id": "jkm7l7i8gWFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import relevant packages\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# We create an instance of the PyPDFLoader by providing the URL of the PDF document we use.\n",
        "# This loader used is specialized for extracting information from PDF files.\n",
        "loader = PyPDFLoader(\"https://www.tlv.se/download/18.12c69789187230f29b822802/1680069871440/report_international_price_comparison_2022_130-2023.pdf\")\n",
        "\n",
        "# We load the document PDF\n",
        "docs = loader.load()\n",
        "# And extrack the lenght / number of pages in the document.\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5PjTS1SAm3",
        "outputId": "321e9233-c323-4f9c-90cc-2fc44709f9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Markdown file5 we're loading is a paper on International price comparison from 2022. The paper was published by Tandvårds- och Läkemedelsförmånsverket, which is the swedish medical and dental institute.\n",
        "\n",
        "Let's see how we can use the RecursiveCharacterTextSplitter to split the document into smaller chunks:"
      ],
      "metadata": {
        "id": "pmUCDibpgdJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import relevant packages\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create an instance of RecursiveCharacterTextSplitter with specific configuration parameters.\n",
        "# In this case, it is configured to split text into chunks of 1024 characters with an overlap of 64 characters between consecutive chunks.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "\n",
        "# We apply the text splitter to split the input document into smaller chunks, as specified in the above line of code.\n",
        "# The split_documents method takes the list of documents and returns a list of text chunks.\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# We get the lenght of the provided list of text chunks.\n",
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0T4HmhYSbUe",
        "outputId": "aa244343-5a78-4b0d-9a1a-392d282c6965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "185"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the document into chunks is required due to the limited number of tokens a LLM can look at once (4096 for Llama 2). Next, we'll use the HuggingFaceEmbeddings class to create embeddings for the chunks:"
      ],
      "metadata": {
        "id": "2Q1EKMLig6oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the relevant packages\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# Create an instance of HuggingFaceEmbeddings with specific configuration parameters.\n",
        "\n",
        "  # * model_name: Specifies the name of the pre-trained Hugging Face model to be used for embeddings (in this case, \"thenlper/gte-large\").\n",
        "\n",
        "  # * model_kwargs: Specifies additional keyword arguments for configuring the underlying model. Here, it sets the device to \"cuda\" for GPU acceleration.\n",
        "\n",
        "  # * encode_kwargs: Specifies additional keyword arguments for the embedding process. Here, it requests normalized embeddings.\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-large\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Use the embed_query method of the embeddings instance to embed the content of the first text chunk (texts[0].page_content).\n",
        "# query_result, will be a representation of the text content in the form of embeddings.\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "\n",
        "# And we print the lenght of it.\n",
        "print(len(query_result))"
      ],
      "metadata": {
        "id": "Y79RVOdTSdCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the spirit of using free tools, we're also using free embeddings hosted by HuggingFace. We'll use Chroma database to store/cache the embeddings and make it easy to search them:\n",
        "\n",
        "To combine the LLM with the database, we'll use the RetrievalQA chain:"
      ],
      "metadata": {
        "id": "7VJOfK1Rg-r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import relevant packages\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Create an instance of the Chroma vector store using the from_documents method. This method takes three parameters:\n",
        "\n",
        "  # * texts: The list of text chunks that you want to store in the vector store.\n",
        "\n",
        "  # * embeddings: The embeddings associated with the text chunks.\n",
        "\n",
        "  # * persist_directory: The directory where the vector store will be stored for future use or persistence.\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "\n",
        "# Use the similarity_search method of the db instance to perform a similarity search. This method takes two parameters:\n",
        "\n",
        "  # * \"falling prices\": The query string for which similarity search is performed.\n",
        "\n",
        "  # * k=2: The number of nearest neighbors (results) to retrieve.\n",
        "\n",
        "  # * The results of the similarity search are stored in the results variable.\n",
        "\n",
        "results = db.similarity_search(\"falling prices\", k=2)\n",
        "\n",
        "# We print Print the content of the first result from the similarity search.\n",
        "# results[0] represents the most similar item found based on the query, and .page_content retrieves the content associated with that result.\n",
        "print(results[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrlF7CXSdW7",
        "outputId": "5c08725e-b6d7-495a-f4ba-14ed2f60f22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the downward trend came to a halt, with relat ive prices remaining 11.2 percent \n",
            "below the average. Using a fixed exchange rate, Sweden's relative prices fell by \n",
            "around ten percentage points up to Q1 2021, and then stabilised at a relative price of \n",
            "2.1 percent above the average. The change over time is  largely driven by the \n",
            "exchange rate change.21 However, from 2014 to 2016, the relative price decrease was \n",
            "mainly driven by reassessments and the introduction of rule -based price reductions \n",
            "for pharmaceuticals 15 years and older. This change can be seen in  Figure 13. As \n",
            "previously mentioned, Sweden's relative prices would likely rise if the Swedish \n",
            "krona regained its value. The following section provides a more detailed description \n",
            "of the reasons behind this price development.  \n",
            " \n",
            "21 A version of the figure with fixed exchange rate is found in Section 1.1 of Appendix 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We (again) import the relevant packages\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template using the PromptTemplate class.\n",
        "# The template includes placeholders {context} and {question} for dynamic input.\n",
        "# The template also provides some instructions and context for answering the question.\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a medical expert. Use the following information to answer the question at the end.\n",
        "<</SYS>>\n",
        "\n",
        "{context}\n",
        "\n",
        "{question} [/INST]\n",
        "\"\"\"\n",
        "# Create an instance of PromptTemplate using the defined template.\n",
        "# We specify the input variables as \"context\" and \"question.\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "#Create an instance of RetrievalQA using the from_chain_type method. This method initializes the QA chain with the following parameters:\n",
        "\n",
        "  # * llm: The language model (llm) to be used in the QA process. As spcified in the block of code in the beginning og the notebook.\n",
        "\n",
        "  # * chain_type: The type of QA chain, specified as \"stuff.\"\n",
        "\n",
        "  # * retriever: The retriever to be used for document retrieval (using the previously created Chroma vector store).\n",
        "\n",
        "  # * return_source_documents: Boolean indicating whether to return the source documents in the result.\n",
        "\n",
        "  # * chain_type_kwargs: Additional keyword arguments specific to the QA chain, including the defined prompt.\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "\n",
        "# Use the qa_chain to perform question-answering.\n",
        "# The input is a question about medicine prices in different countries.\n",
        "# The result contains the answer generated by the QA system.\n",
        "result = qa_chain(\n",
        "    \"Explain how some of the different countries compare when it comes to medicine prices.\"\n",
        ")\n",
        "\n",
        "# We print the results\n",
        "print(result[\"result\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxkw38PzSfhq",
        "outputId": "f3fab61e-dad2-4645-8e7d-02a5d4968a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a medical expert, I can provide you with an analysis of how different countries compare when it comes to medicine prices based on the provided information.\n",
            "Firstly, it is important to note that the global pharmaceutical market is highly consolidated, with a few major players holding significant market share. This means that pricing strategies and policies can vary greatly between countries, depending on their economic and regulatory frameworks.\n",
            "In terms of medicine prices, the data suggests that North America dominates the pharmaceutical market, accounting for 49.1% of total sales in the global market. This is likely due to several factors, including the presence of large pharmaceutical companies in the region, a well-developed healthcare system, and a relatively high average income. In contrast, Europe accounts for 23.4% of global pharmaceutical sales, while Africa, Asia (excluding Japan and China), and Australia combine for 8.4%. China accounts for 9.4%, Japan for 6.1%, and Latin America for 3.7%.\n",
            "When comparing the prices of medicines across countries, we see that Sweden has one of the highest prices globally, with an average price of SEK 10,787 billion (approximately $1.1 trillion USD) calculated as the price from the manufacturer. This is likely due to several factors, including the country's well-developed healthcare system, strong regulations around drug pricing, and a relatively high average income.\n",
            "On the other hand, countries like China and India have much lower prices for medicines, with an average price of RMB 5,677 billion (approximately $830 billion USD) and INR 1,034 billion (approximately $14 billion USD), respectively. This is likely due to a combination of factors, including lower production costs, less stringent regulations around drug pricing, and a larger domestic market.\n",
            "It is worth noting that currency exchange rates can also play a significant role in determining medicine prices across countries. For example, over the past few years, the Swedish krona has fallen in value against the euro, which could contribute to higher medicine prices in Sweden relative to other countries. Similarly, countries with weaker economies may face challenges in affording expensive medications due to limited financial resources.\n",
            "In conclusion, while there are many factors that influence medicine prices across countries, the data suggests that North America has the highest prices globally, followed by Europe and Switzerland. On the other hand, countries like China and India have significantly lower prices for medicines, likely due to a combination of factors including lower production costs, less stringent regulations around drug pricing, and a larger domestic market. However, currency exchange rates and other economic factors can also impact medicine prices across countries, making direct comparisons challenging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will pass our prompt to the LLM along with the top 2 results from the database. The LLM will then use the prompt to generate an answer. The answer will be returned along with the source documents. Let's try another prompt:"
      ],
      "metadata": {
        "id": "jnlZOen-hFh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We (again again) import relevant packages\n",
        "from textwrap import fill\n",
        "\n",
        "# Use the qa_chain to perform question-answering with a new question.\n",
        "# The input is a question related to summarizing how countries determine the costs of medicine.\n",
        "# The result contains the generated answer by the QA system.\n",
        "result = qa_chain(\n",
        "    \"Summarize the different ways countries decide on costs of medicin\"\n",
        ")\n",
        "# Print the generated answer after applying text wrapping.\n",
        "# The fill function from the textwrap module is used to format the text, ensuring that lines do not exceed a specified width of 80 characters.\n",
        "print(fill(result[\"result\"].strip(), width=80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUlORyR8ShP1",
        "outputId": "c5814ba0-292d-4bca-b1f8-896289c38cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a medical expert, I can provide you with an analysis of the different ways\n",
            "countries decide on the costs of medicines based on the provided text. Here are\n",
            "some key points: 1. Price comparison: Countries compare the prices of\n",
            "pharmaceuticals across different markets to determine the relative cost of\n",
            "drugs. However, there are challenges in grouping pharmaceuticals according to\n",
            "therapy groups, and treatment traditions may vary between countries. 2.\n",
            "Exclusion of low-volume pharmaceuticals: Countries exclude pharmaceuticals with\n",
            "very low volumes in their domestic market to prevent disproportionate weight in\n",
            "the price comparison. 3. Volume data: Sales volumes for the previous 12 months\n",
            "up to March 2022 are used in calculating bilateral indices. 4. Transparency in\n",
            "list prices: Different countries have varying levels of transparency in list\n",
            "prices, including whether or not discount systems are institutionalized and\n",
            "reflected in list prices. 5. Global market share: Globally, North America\n",
            "dominates the pharmaceutical market, accounting for 49.1% of total sales, while\n",
            "Europe accounts for 23.4%, Africa, Asia (excluding Japan and China), Australia,\n",
            "China, Japan, and Latin America each account for less than 10%. 6. Currency\n",
            "exchange rates: The Swedish krona has depreciated against the euro for several\n",
            "years, affecting the cost of medicines in Sweden. In summary, countries use\n",
            "various methods to determine the costs of medicines, including comparing prices\n",
            "across markets, excluding low-volume pharmaceuticals, using volume data,\n",
            "considering transparency in list prices, analyzing global market shares, and\n",
            "taking currency exchange rates into account. These approaches help countries\n",
            "make informed decisions about drug pricing and ensure fair competition among\n",
            "pharmaceutical companies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment we've implemented a NLP pipeline using the LangChain library and Hugging Face Transformers. Initially, we set up a Question-Answering (QA) system employing a RetrievalQA chain with a predefined template for medical expertise using a document, that explains medical pricing mechanisms in a range of countries. This system utilized a language model (llm) and a document vector store (Chroma) for efficient retrieval and processing. We then demonstrated the system's capability by answering questions related to medicine prices.\n",
        "\n",
        "Additionally, we integrated text splitting to handle large documents efficiently and embedded the text content using a pre-trained language model.\n",
        "\n",
        "Furthermore, we performed similarity search to find relevant information based on user queries. Finally, we summarized the results and presented them with proper text wrapping for readability. The overall system allows for querying, retrieval, and summarization of information."
      ],
      "metadata": {
        "id": "-s6N5fdvmy2R"
      }
    }
  ]
}